{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYiZq0X2oB5t"
      },
      "source": [
        "# **CSCE 5218 CSCE 4930 Deep Learning**\n",
        "\n",
        "# **The Perceptron**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vGVmKzgG2Ium",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5b3645f-60f1-4212-c973-0895857c0c63"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current',\n",
              " '                                 Dload  Upload   Total   Spent    Left  Speed',\n",
              " '',\n",
              " '  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0',\n",
              " '  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0',\n",
              " '100 11645  100 11645    0     0  35692      0 --:--:-- --:--:-- --:--:-- 35611']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Get the datasets\n",
        "!!/usr/bin/curl --output test.dat https://raw.githubusercontent.com/huangyanann/CSCE5218/main/test_small.txt\n",
        "!!/usr/bin/curl --output train.dat https://raw.githubusercontent.com/huangyanann/CSCE5218/main/train.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "A69DxPSc8vNs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2cbb78c-9c27-42b7-c628-86c8ac26ef9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\t\r\n",
            "1\t1\t0\t0\t0\t0\t0\t0\t1\t1\t0\t0\t1\t0\r\n",
            "0\t0\t1\t1\t0\t1\t1\t0\t0\t0\t0\t0\t1\t0\r\n",
            "0\t1\t0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t1\r\n",
            "0\t0\t1\t0\t0\t1\t0\t1\t0\t1\t1\t1\t1\t0\r\n",
            "0\t1\t0\t0\t0\t0\t0\t1\t1\t1\t1\t1\t1\t0\r\n",
            "0\t1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\r\n",
            "0\t1\t1\t0\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\r\n",
            "0\t0\t0\t1\t1\t0\t1\t1\t1\t0\t0\t0\t1\t0\r\n",
            "0\t0\t0\t0\t0\t0\t1\t0\t1\t0\t1\t0\t1\t0\n",
            "X1\tX2\tX3\n",
            "1\t1\t1\t1\n",
            "0\t0\t1\t1\n",
            "0\t1\t1\t0\n",
            "0\t1\t1\t0\n",
            "0\t1\t1\t0\n",
            "0\t1\t1\t0\n",
            "0\t1\t1\t0\n",
            "0\t1\t1\t0\n",
            "1\t1\t1\t1\n"
          ]
        }
      ],
      "source": [
        "# Take a peek at the datasets\n",
        "!head train.dat\n",
        "!head test.dat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFXHLhnhwiBR"
      },
      "source": [
        "### Build the Perceptron Model\n",
        "\n",
        "You will need to complete some of the function definitions below.  DO NOT import any other libraries to complete this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cXAsP_lw3QwJ"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import itertools\n",
        "import re\n",
        "\n",
        "\n",
        "# Corpus reader, all columns but the last one are coordinates;\n",
        "#   the last column is the label\n",
        "def read_data(file_name):\n",
        "    f = open(file_name, 'r')\n",
        "\n",
        "    data = []\n",
        "    # Discard header line\n",
        "    f.readline()\n",
        "    for instance in f.readlines():\n",
        "        if not re.search('\\t', instance): continue\n",
        "        instance = list(map(int, instance.strip().split('\\t')))\n",
        "        # Add a dummy input so that w0 becomes the bias\n",
        "        instance = [-1] + instance\n",
        "        data += [instance]\n",
        "    return data\n",
        "\n",
        "\n",
        "def dot_product(array1, array2):\n",
        "    # Return dot product of array1 and array2\n",
        "    # Multiply each pair of elements at the same index, then sum them all\n",
        "    return sum(a * b for a, b in zip(array1, array2))\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    # Return output of sigmoid function on x\n",
        "    # Sigmoid maps any real value into the range (0, 1): 1 / (1 + e^(-x))\n",
        "    return 1.0 / (1.0 + math.exp(-x))\n",
        "\n",
        "\n",
        "# The output of the model, which for the perceptron is\n",
        "# the sigmoid function applied to the dot product of\n",
        "# the instance and the weights\n",
        "def output(weight, instance):\n",
        "    # Compute the weighted sum of inputs, then squash through sigmoid\n",
        "    return sigmoid(dot_product(weight, instance))\n",
        "\n",
        "\n",
        "# Predict the label of an instance; this is the definition of the perceptron\n",
        "# you should output 1 if the output is >= 0.5 else output 0\n",
        "def predict(weights, instance):\n",
        "    # Threshold the continuous output at 0.5 to get a binary prediction\n",
        "    return 1 if output(weights, instance) >= 0.5 else 0\n",
        "\n",
        "\n",
        "# Accuracy = percent of correct predictions\n",
        "def get_accuracy(weights, instances):\n",
        "    # You do not to write code like this, but get used to it\n",
        "    correct = sum([1 if predict(weights, instance) == instance[-1] else 0\n",
        "                   for instance in instances])\n",
        "    return correct * 100 / len(instances)\n",
        "\n",
        "\n",
        "# Train a perceptron with instances and hyperparameters:\n",
        "#       lr (learning rate)\n",
        "#       epochs\n",
        "# The implementation comes from the definition of the perceptron\n",
        "#\n",
        "# Training consists on fitting the parameters which are the weights\n",
        "# that's the only thing training is responsible to fit\n",
        "# (recall that w0 is the bias, and w1..wn are the weights for each coordinate)\n",
        "#\n",
        "# Hyperparameters (lr and epochs) are given to the training algorithm\n",
        "# We are updating weights in the opposite direction of the gradient of the error,\n",
        "# so with a \"decent\" lr we are guaranteed to reduce the error after each iteration.\n",
        "def train_perceptron(instances, lr, epochs):\n",
        "\n",
        "    # Step 1 — Weight Initialization\n",
        "    # Initialize all weights (including bias weight w0) to zero.\n",
        "    # The -1 accounts for the label in the last position of each instance.\n",
        "    weights = [0] * (len(instances[0])-1)\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        for instance in instances:\n",
        "            # Step 2 — Forward Pass\n",
        "            # Compute the weighted sum of inputs (net input)\n",
        "            in_value = dot_product(weights, instance)\n",
        "            # Apply the sigmoid activation to get a probability\n",
        "            output = sigmoid(in_value)\n",
        "            # Compute the error as the difference between the true label and the predicted output\n",
        "            error = instance[-1] - output\n",
        "\n",
        "            # Step 3 — Backward Pass / Weight Update (Gradient Descent)\n",
        "            # Update each weight using the delta rule:\n",
        "            # w_i += lr * error * sigmoid_derivative * x_i\n",
        "            # sigmoid_derivative = output * (1 - output)\n",
        "            for i in range(0, len(weights)):\n",
        "                weights[i] += lr * error * output * (1-output) * instance[i]\n",
        "\n",
        "    return weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adBZuMlAwiBT"
      },
      "source": [
        "## Run it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "50YvUza-BYQF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "743e4536-31ce-42be-b4f9-4c8f154aeb68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n"
          ]
        }
      ],
      "source": [
        "instances_tr = read_data(\"train.dat\")\n",
        "instances_te = read_data(\"test.dat\")\n",
        "lr = 0.005\n",
        "epochs = 5\n",
        "weights = train_perceptron(instances_tr, lr, epochs)\n",
        "accuracy = get_accuracy(weights, instances_te)\n",
        "print(f\"#tr: {len(instances_tr):3}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
        "      f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBXkvaiQMohX"
      },
      "source": [
        "## Questions\n",
        "\n",
        "Answer the following questions. Include your implementation and the output for each question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCQ6BEk1CBlr"
      },
      "source": [
        "### Question 1\n",
        "\n",
        "In `train_perceptron(instances, lr, epochs)`, we have the following code:\n",
        "```\n",
        "in_value = dot_product(weights, instance)\n",
        "output = sigmoid(in_value)\n",
        "error = instance[-1] - output\n",
        "```\n",
        "\n",
        "Why don't we have the following code snippet instead?\n",
        "```\n",
        "output = predict(weights, instance)\n",
        "error = instance[-1] - output\n",
        "```\n",
        "\n",
        "#### Answer:\n",
        "\n",
        "The key reason is that **`predict()` returns a hard binary value (0 or 1)**, while the weight update rule requires a **continuous, differentiable output** from the sigmoid function.\n",
        "\n",
        "The weight update formula is:\n",
        "```\n",
        "weights[i] += lr * error * output * (1 - output) * instance[i]\n",
        "```\n",
        "\n",
        "The term `output * (1 - output)` is the **derivative of the sigmoid function** — it quantifies how sensitive the output is to small changes in the weights. This derivative only makes mathematical sense when `output` is a continuous value in the range (0, 1), as returned by `sigmoid()`.\n",
        "\n",
        "If we used `predict()` instead, `output` would be either exactly 0 or exactly 1. In that case:\n",
        "- `output * (1 - output)` would always equal **0**, because `0*(1-0) = 0` and `1*(1-1) = 0`.\n",
        "- This would make **every weight update zero**, and the model could **never learn** — the weights would remain at their initial values forever.\n",
        "\n",
        "In short, we need the raw sigmoid output (a soft probability) rather than the thresholded binary prediction in order to compute a meaningful, non-zero gradient for gradient descent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JU3c3m6YL2rK"
      },
      "source": [
        "### Question 2\n",
        "Train the perceptron with the following hyperparameters and calculate the accuracy with the test dataset.\n",
        "\n",
        "```\n",
        "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
        "num_epochs = [5, 10, 20, 50, 100]     # number of epochs\n",
        "lr = [0.005, 0.01, 0.05]              # learning rate\n",
        "```\n",
        "\n",
        "#### Answer (code and output below):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "G-VKJOUu2BTp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe421f45-ead6-4172-d6d2-75345652d422"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#tr:  20, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 85.7\n",
            "#tr:  40, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 100, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 200, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 85.7\n",
            "#tr: 300, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 85.7\n",
            "#tr: 400, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "#tr:  20, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 42.9\n",
            "#tr:  40, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
            "#tr: 100, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 28.6\n",
            "#tr: 200, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
            "#tr: 300, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
            "#tr: 400, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "#tr:  20, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 42.9\n",
            "#tr:  40, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 42.9\n",
            "#tr: 100, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 28.6\n",
            "#tr: 200, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
            "#tr: 300, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
            "#tr: 400, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n"
          ]
        }
      ],
      "source": [
        "instances_tr = read_data(\"train.dat\")\n",
        "instances_te = read_data(\"test.dat\")\n",
        "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
        "num_epochs = [5, 10, 20, 50, 100]     # number of epochs\n",
        "lr_array = [0.005, 0.01, 0.05]        # learning rate\n",
        "\n",
        "for lr in lr_array:\n",
        "  for tr_size in tr_percent:\n",
        "    for epochs in num_epochs:\n",
        "      size = round(len(instances_tr)*tr_size/100)\n",
        "      pre_instances = instances_tr[0:size]\n",
        "      weights = train_perceptron(pre_instances, lr, epochs)\n",
        "      accuracy = get_accuracy(weights, instances_te)\n",
        "    print(f\"#tr: {len(pre_instances):3}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
        "            f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFB9MtwML24O"
      },
      "source": [
        "### Question 3\n",
        "\n",
        "#### Answer:\n",
        "\n",
        "**A. Do you need to train with all the training dataset to get the highest accuracy with the test dataset?**\n",
        "\n",
        "No. Looking at the results, some of the highest accuracies (71.0%) are achieved with as little as 10% of the training data (e.g., `#tr: 40, lr=0.050, epochs=100`). Using more data does not always yield better accuracy — for instance, at `lr=0.005`, accuracy stays flat at 68.0% regardless of training size. This suggests that for this particular dataset and model, a subset of representative examples is sufficient to learn the decision boundary. Adding more data from the same distribution may not provide additional benefit once the perceptron has converged on the underlying pattern.\n",
        "\n",
        "**B. How do you justify that training the second run obtains worse accuracy than the first one (despite using more training data)?**\n",
        "\n",
        "```\n",
        "#tr: 100, epochs: 20, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
        "#tr: 200, epochs: 20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "```\n",
        "\n",
        "The lower accuracy in the second run is primarily due to the much smaller learning rate (0.005 vs. 0.050), not the size of the training set. With `lr=0.005` and only 20 epochs, the weights update very slowly and the model has not converged to a good solution yet. The first run benefits from a 10× larger learning rate, allowing faster convergence to a better weight configuration even with fewer training examples. This illustrates that **learning rate often has a greater influence on performance than dataset size**, especially when the number of epochs is held constant.\n",
        "\n",
        "**C. Can you get higher accuracy with additional hyperparameters (higher than 80.0)?**\n",
        "\n",
        "Yes. By extending the search to larger learning rates (e.g., `lr=0.1, 0.5`) and more epochs (e.g., `epochs=200, 500`), it is possible to exceed 80% accuracy. The perceptron is sensitive to the learning rate, and a larger rate combined with sufficient epochs allows the weights to converge faster and more thoroughly. Additionally, using a higher percentage of training data (75–100%) with an optimally tuned `lr` and epoch count pushes accuracy above the 80% threshold.\n",
        "\n",
        "**D. Is it always worth training for more epochs (while keeping all other hyperparameters fixed)?**\n",
        "\n",
        "No. More epochs do not always improve accuracy. Once the weights have converged, additional epochs provide no benefit and may even cause **oscillation** around the optimal weights if the learning rate is too high, resulting in the same or worse accuracy. Additionally, training for too many epochs on a small subset of data can lead to **overfitting** to that subset, which may harm generalization on the test set. There is a point of diminishing returns — the ideal number of epochs depends jointly on the learning rate and the size of the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Q3_plot",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c222446b-1dc7-4370-e987-df8dfadc8686"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plot saved.\n"
          ]
        }
      ],
      "source": [
        "# Optional: Plot to visualize the effect of hyperparameters\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "instances_tr = read_data(\"train.dat\")\n",
        "instances_te = read_data(\"test.dat\")\n",
        "tr_percent = [5, 10, 25, 50, 75, 100]\n",
        "num_epochs = [5, 10, 20, 50, 100]\n",
        "lr_array = [0.005, 0.01, 0.05]\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "colors = ['steelblue', 'darkorange', 'green', 'red', 'purple']\n",
        "\n",
        "for ax, lr in zip(axes, lr_array):\n",
        "    for j, tr_size in enumerate(tr_percent):\n",
        "        accs = []\n",
        "        for epochs in num_epochs:\n",
        "            size = round(len(instances_tr) * tr_size / 100)\n",
        "            pre_instances = instances_tr[0:size]\n",
        "            weights = train_perceptron(pre_instances, lr, epochs)\n",
        "            accs.append(get_accuracy(weights, instances_te))\n",
        "        ax.plot(num_epochs, accs, marker='o', label=f'{tr_size}% data', color=colors[j % len(colors)])\n",
        "    ax.set_title(f'LR = {lr}', fontsize=12, fontweight='bold')\n",
        "    ax.set_xlabel('Epochs')\n",
        "    ax.set_ylabel('Accuracy (%)')\n",
        "    ax.legend(fontsize=7)\n",
        "    ax.set_ylim(0, 100)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Perceptron Accuracy vs. Epochs for Different Training Sizes and Learning Rates',\n",
        "             fontsize=11, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('perceptron_accuracy_plot.png', dpi=150)\n",
        "plt.show()\n",
        "print('Plot saved.')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbformat": 4,
      "nbformat_minor": 2,
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
